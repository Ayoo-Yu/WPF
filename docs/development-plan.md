# 风电功率预测赛马场开发计划（V1）

## 1. 计划目标

在保证可复现与可扩展的前提下，把当前 Demo 演进为可用于真实科研迭代的平台，覆盖：

1. 真实数据接入  
2. 多模型统一回测与超参搜索  
3. 分层评估与稳定性分析  
4. 前端可视化与实验管理  
5. 基础工程质量（测试、日志、配置、可部署）

---

## 2. 开发节奏与时间线

建议按 8 周执行，起始时间按 **2026-02-16（周一）** 计算。

| 阶段 | 日期 | 目标 |
|---|---|---|
| P0 需求冻结与规范 | 2026-02-16 ~ 2026-02-20 | 固化任务定义、数据口径、指标口径 |
| P1 数据与回测内核 | 2026-02-23 ~ 2026-03-06 | 接入真实数据，跑通真实回测链路 |
| P2 模型与搜索 | 2026-03-09 ~ 2026-03-20 | 接入核心模型与统一超参搜索 |
| P3 评估与排行榜 | 2026-03-23 ~ 2026-04-03 | 建立分层评估、稳定性榜单 |
| P4 前端产品化 | 2026-04-06 ~ 2026-04-17 | 实验管理页面、图表与筛选完善 |
| P5 质量与可运维 | 2026-04-20 ~ 2026-04-24 | 测试、监控、容错、部署脚本 |
| P6 验收与试运行 | 2026-04-27 ~ 2026-05-01 | 试运行与迭代清单 |

---

## 3. 分阶段详细任务

## P0：需求冻结与规范（1 周）

交付物：
- `task_type` 标准定义（超短期/短期/中期）  
- `horizon` 统一口径表  
- 指标口径文档（MAE/RMSE/nMAE/MAPE 使用边界）  
- 数据字段字典（功率、气象、时间戳、限电标记）

关键任务：
1. 完成真实业务口径确认（场站、时区、采样频率、缺失处理）。  
2. 固化配置文件规范（实验、模型、切分、评估）。  
3. 定义“实验最小复现信息”字段（代码版本、数据版本、随机种子、依赖版本）。

验收标准：
1. 任意实验需求都能映射到统一 YAML。  
2. 口径文档可独立指导新成员复现实验。  

---

## P1：数据与回测内核（2 周）

交付物：
- 真实数据读取器与预处理模块  
- 时间因果安全的特征流水线  
- 滚动回测引擎（rolling/expanding）  
- 数据版本登记机制（dataset_version）

关键任务：
1. 接入真实数据源（先支持 1~3 个风电场）。  
2. 实现缺失值、异常值、时间对齐处理。  
3. 保证无泄漏切分。  
4. 输出统一训练样本结构，兼容后续模型。

验收标准：
1. 指定风场可一键跑完整回测。  
2. 回测日志可追踪每个窗口起止时间。  
3. 手工抽检无未来信息泄漏。  

---

## P2：模型与超参搜索（2 周）

交付物：
- 模型插件化接入规范  
- 3 类模型可运行：持久性基线、树模型（LightGBM/XGBoost）、时序深度模型（LSTM）  
- 统一搜索器（Grid + Random，后续可扩展 Bayesian）

关键任务：
1. 在 `ForecastModel` 统一接口下接入模型。  
2. 实现模型训练产物保存与加载。  
3. 增加搜索并行执行与失败重试。  
4. 记录每次 trial 的参数、耗时、指标。

验收标准：
1. 同一实验内可批量比较多个模型。  
2. 失败 trial 不影响整体任务完成。  
3. 可查询“某风场某任务最佳参数组合”。  

---

## P3：评估与排行榜（2 周）

交付物：
- 多维评估引擎（整体、按 horizon、按季节、按风速区间）  
- 排行榜（性能榜 + 稳定性榜）  
- 报告模板（Markdown/CSV）

关键任务：
1. 实现多指标聚合规则。  
2. 实现分层切片评估。  
3. 增加稳定性指标（方差/分位数表现）。  
4. 建立“候选模型库”（best-by-site/task）。

验收标准：
1. 可输出场站级 TopN。  
2. 可识别“平均好但不稳定”的模型。  
3. 报告可直接用于组会或论文实验记录。  

---

## P4：前端产品化（2 周）

交付物：
- 实验创建与运行页面  
- 历史运行管理页面  
- 图表中心（指标趋势、模型对比、场站筛选）  
- 基础结果导出（CSV）

关键任务：
1. 把当前 Demo 页面升级为多页面结构。  
2. 增加筛选器：场站、任务类型、时间范围、指标。  
3. 增加图表交互：缩放、切换指标、下载。  
4. 增加运行状态展示（运行中/完成/失败）。

验收标准：
1. 不写命令行也能完成常见实验流程。  
2. 前端筛选结果与后端统计一致。  
3. 核心操作在桌面端与移动端都可用。  

---

## P5：质量与可运维（1 周）

交付物：
- 单元测试与集成测试  
- CI 流水线（至少 lint + test）  
- 日志规范、错误码与告警基础  
- 本地部署脚本（一键启动）

关键任务：
1. 为核心模块补测试（数据切分、评估、排行榜、API）。  
2. 建立最小 CI。  
3. 增加异常处理与运行超时控制。  
4. 完成运行手册（启动、排错、恢复）。

验收标准：
1. 核心逻辑测试覆盖关键路径。  
2. 主要错误可定位可恢复。  
3. 新环境可按文档完成部署。  

---

## P6：验收与试运行（1 周）

交付物：
- 试运行报告（至少 3 个风场、3 类任务）  
- 问题清单与下一版本路线图  
- V1 发布说明

关键任务：
1. 做端到端演练并记录耗时。  
2. 对比旧流程，量化效率提升。  
3. 输出 V1.1 优化 backlog。

验收标准：
1. 可稳定支持日常研究对比实验。  
2. 能快速回答“当前最优模型是谁、稳定性如何”。  

---

## 4. 并行开发流（建议）

为提高效率，建议按 3 条线并行：

1. 数据与回测线：数据接入、切分、特征、运行调度。  
2. 模型与评估线：模型插件、搜索器、指标、排行榜。  
3. 前端与产品线：页面、API、可视化、用户交互。  

每周合并一次主分支稳定版本，避免长期分叉。

---

## 5. 每周例行机制

每周固定两个节奏：

1. 周一：确认本周目标、冻结任务。  
2. 周五：演示可运行结果、记录偏差与风险。  

每周必须产出：
1. 1 份可运行版本。  
2. 1 份简短周报（完成项、阻塞项、下周计划）。  

---

## 6. 任务优先级（执行顺序）

1. P0 + P1（真实数据 + 无泄漏回测）  
2. P2（核心模型 + 搜索）  
3. P3（分层评估 + 稳定性）  
4. P4（前端完善）  
5. P5 + P6（质量与验收）  

原则：先保证“结果可信”，再提升“交互体验”。

---

## 7. 风险与预案

1. 数据质量波动：提前建立数据校验与缺失告警。  
2. 训练资源不足：支持子集回测与夜间批处理。  
3. 实验过多难管理：强制实验命名规范与标签体系。  
4. 指标冲突：固定主指标 + 辅指标，避免频繁变口径。  

---

## 8. 立即执行清单（下周）

下一个工作周（2026-02-16 ~ 2026-02-20）建议直接执行：

1. 冻结 `task_type/horizon/metrics` 口径文档。  
2. 完成真实数据样例接入（至少 1 个场站）。  
3. 在现有 demo 上跑通“真实数据 + persistence”完整链路。  
4. 输出第一版真实数据回测报告（CSV + 页面可查看）。

